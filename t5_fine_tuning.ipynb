{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kremerss/FOM-LaTeX-Template/blob/main/t5_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPJyaa1P0teM",
        "outputId": "9adb95ac-7ccd-4086-dfd0-909e56024d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf5QCQGU0wFB"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch_lightning\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "def build_20newsgroups_dataset(): #-> datasets.Dataset:\n",
        "  newsgroups = fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "  df = pd.DataFrame(newsgroups.data, columns=['text'])\n",
        "  df['label_id'] = pd.Series(data=newsgroups.target)\n",
        "\n",
        "  df_target_names = pd.DataFrame(data=newsgroups.target_names, columns=['label'])\n",
        "  df = pd.merge(df, df_target_names, left_on='label_id', right_index=True).sort_index()\n",
        "\n",
        "  cat = {\n",
        "    'religion': [0,15,19]\n",
        "    ,'tech': list(range(1,6))\n",
        "    ,'misc': [6]    \n",
        "    ,'sports': list(range(7,11))\n",
        "    ,'science': list(range(11,15))\n",
        "    , 'politics': list(range(16,19))\n",
        "  }\n",
        "\n",
        "  for key, value in cat.items():\n",
        "    df['label'].loc[df['label_id'].isin(value)] = key\n",
        "\n",
        "  df.drop('label_id', axis=1, inplace=True)\n",
        "\n",
        "  #return datasets.Dataset.from_pandas(df,preserve_index=False)\n",
        "  return df"
      ],
      "metadata": {
        "id": "BOwci52wgmK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  cat = {\n",
        "    'religion': [0,15,19]\n",
        "    ,'tech': list(range(1,6))\n",
        "    ,'misc': [6]    \n",
        "    ,'sports': list(range(7,11))\n",
        "    ,'science': list(range(11,15))\n",
        "    , 'politics': list(range(16,19))\n",
        "  }\n",
        "\n",
        "tokens = []\n",
        "for key, value in cat.items():\n",
        "    tokens.append(tokenizer(key))\n",
        "max(len(token) for token in tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--V8lNFAhf1v",
        "outputId": "0a4a4574-fe83-4cb7-b71a-f5fa0f1a59df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import argparse\n",
        "import logging\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset as hugDataset\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "NelLiiLe_UW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.getLevelName(\"INFO\"),\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        ")"
      ],
      "metadata": {
        "id": "9JO6Vwq7NNFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customDataset(Dataset):\n",
        "  def __init__(self, df, tokenizer, max_len=512):\n",
        "\n",
        "    self.data = hugDataset.from_pandas(df, preserve_index=False)\n",
        "    #print(self.data.head())\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.inputs = []\n",
        "    self.targets = []\n",
        "\n",
        "    self._build()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.inputs)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    source_ids = self.inputs[idx]['input_ids'].squeeze()\n",
        "    target_ids = self.targets[idx]['input_ids'].squeeze()\n",
        "\n",
        "    src_mask = self.inputs[idx]['attention_mask'].squeeze()\n",
        "    trg_mask = self.targets[idx]['attention_mask'].squeeze()\n",
        "\n",
        "    return {'source_ids': source_ids, 'target_ids': target_ids, 'source_mask': src_mask, 'target_mask': trg_mask}\n",
        "\n",
        "  def _build(self):\n",
        "    for idx in range(len(self.data)):\n",
        "      input_, target = self.data['text'][idx], self.data['label'][idx]\n",
        "\n",
        "      # tokenize inputs\n",
        "      tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "          [input_]\n",
        "          , max_length=self.max_len\n",
        "          , padding='max_length'\n",
        "          , truncation=True\n",
        "          , return_tensors=\"pt\"\n",
        "      )\n",
        "      # tokenize targets\n",
        "      tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "          [target]\n",
        "          , max_length=2\n",
        "          , padding='max_length'\n",
        "          , truncation=True\n",
        "          , return_tensors=\"pt\"\n",
        "      )\n",
        "      self.inputs.append(tokenized_inputs)\n",
        "      self.targets.append(tokenized_targets)"
      ],
      "metadata": {
        "id": "isUNKzE9ZVXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestClass(pl.LightningModule):\n",
        "  def __init__(self,df):\n",
        "    logger.info('T5FineTuner init')\n",
        "    super(TestClass, self).__init__()\n",
        "\n",
        "\n",
        "    cuda = torch.device('cuda')\n",
        "    self.df_train, self.df_test = train_test_split(df, test_size=.2)\n",
        "\n",
        "a = TestClass(build_20newsgroups_dataset())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tu7P7hWPjLR",
        "outputId": "019ffc77-4ff9-4a7c-de99-1ccb448b9d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_block(indexer, value, name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "data = customDataset(a.df_train.head(),tokenizer=tokenizer)\n",
        "\n",
        "data.__getitem__(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTa_Z1p9Q184",
        "outputId": "35221a38-c498-46c6-e035-d19a06a4394e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source_ids': tensor([   27,   183,  1119,    12,  1431,    46,  1023,  1831,   478,    24,\n",
              "          2284,     8,     3, 12604,  2471,  2594,  4924,     5,    37,  2471,\n",
              "          2594,  5508,  2347, 19768,    11,  5223,    12,     8,   433,    28,\n",
              "           150,   682,     5,   299,     8,   478, 22298,    44,     8,   166,\n",
              "           580,    12,     3,     4, 10499,    51,   345,    76,    17, 29979,\n",
              "             6,    28,     8,   826,  1569,    10,     3,     4,   848,    52,\n",
              "           127,    13,  4567,  1690,    10,  3862, 10499,    51,   134,    15,\n",
              "           122,    41,    77, 27769,  2471,  5508, 15577,    61,  9236,     3,\n",
              "            32,   102,  4978,    13,  4567,  1690,    10,     3, 22974,    41,\n",
              "         12604,    18,  9122,   329,    61, 20650,     3,    32,   102,  4978,\n",
              "            13,  4567,  1690,    10,   220,    41,     4,   834, 10499,    51,\n",
              "           345,    76,    17, 29979,    61, 15696,   297,     3,    23,    26,\n",
              "            16,  4567,  1690,     3,   632,   226,   632, 29549,   381,    13,\n",
              "          4567,  1690,    10,   489,  4853, 12892, 10501,   381,    16,  3911,\n",
              "          6093,    10,   489,  4165,  2792,    27,   243,     6,    27,   410,\n",
              "          3505,  6450,    30,    66,     8,  3088,    12,  6660,    51,  2782,\n",
              "            11,  6660,  3357,    24,    33,  1316,    12,   482,     8,  2471,\n",
              "          2594,  5508,     6,    38,   168,    38,  6450,     3,     4, 10499,\n",
              "            51,   188,    17,    17,  1836,     5,   290,    33,   150,   982,\n",
              "             5,   156, 13112,    65,   141,     8,   337,   682,    42,    65,\n",
              "           261,     3, 12604,    18,  9122,   329,   406,   578,     8,   337,\n",
              "           682,     6,   754,   752,   140,   214,     5,   938,     8,   194,\n",
              "             6,    27,   183,  1180,  2384, 28265,  1877,   632,    30,     3,\n",
              "             9,  3068, 13063,    75,  4416,     1,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]),\n",
              " 'target_ids': tensor([5256,    1]),\n",
              " 'source_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'target_mask': tensor([1, 1])}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class T5FineTuner(pl.LightningModule):\n",
        "  def __init__(self,df, hparams):\n",
        "    logger.info('T5FineTuner init')\n",
        "    super(T5FineTuner, self).__init__()\n",
        "\n",
        "\n",
        "    cuda = torch.device('cuda')\n",
        "    self.df_train, self.df_test = train_test_split(df, test_size=.2)\n",
        "    self.hyparams = hparams\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name).to(device=cuda)\n",
        "    self.tokenizer = T5Tokenizer.from_pretrained(hparams.model_name, model_max_length=hparams.max_len)#.to(device=cuda)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None):\n",
        "      # Function for performing the calculation of the module.\n",
        "      logger.info('forward')\n",
        "      return self.model(\n",
        "          input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          decoder_input_ids=decoder_input_ids,\n",
        "          decoder_attention_mask=decoder_attention_mask,\n",
        "          labels=lm_labels,\n",
        "      )\n",
        "\n",
        "  def _step(self, batch):\n",
        "      logger.info('_step')\n",
        "      # Get target ids\n",
        "      lm_labels = batch['target_ids']\n",
        "      # Ignore padding token within the lost function  (-100 = ignore_index of the CrossEntropyLoss)\n",
        "      lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "      # Generate inferences for the batch\n",
        "      outputs = self(\n",
        "          input_ids=batch['source_ids'],\n",
        "          attention_mask=batch['source_mask'],\n",
        "          lm_labels=lm_labels,\n",
        "          decoder_attention_mask=batch['target_mask']\n",
        "      )\n",
        "\n",
        "      # Get batches loss\n",
        "      loss = outputs[0]\n",
        "\n",
        "      return loss\n",
        "\n",
        "  def training_step(self,batch,batch_idx):\n",
        "    logger.info('training_step')\n",
        "    loss = self._step(batch)\n",
        "    self.log('loss',loss)\n",
        "\n",
        "    return({'loss':loss})\n",
        "\n",
        "  def training_epochs_end(self,outputs):\n",
        "    logger.info('training_epochs_end')\n",
        "    avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "    self.log('avg_train_loss', avg_loss)\n",
        "\n",
        "  def validation_step (self, batch, batch_idx):\n",
        "    logger.info('validation_step')\n",
        "    loss = self._step(batch)\n",
        "    self.log('val_loss', loss)\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    logger.info('configure_optimizers')\n",
        "    model = self.model\n",
        "    no_decay = ['bias','LayerNorm.weight']\n",
        "\n",
        "    optimizers_grouped_parameters = [\n",
        "        {\n",
        "            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)]\n",
        "            , 'weight_decay': self.hyparams.weight_decay\n",
        "        },\n",
        "        {\n",
        "            'params': [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)]\n",
        "            , 'weight_decay': 0.0\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizers_grouped_parameters\n",
        "                      , lr=self.hyparams.learning_rate\n",
        "                      , eps=self.hyparams.adam_epsilon)\n",
        "\n",
        "    self.opt = optimizer\n",
        "\n",
        "    return [optimizer]\n",
        "\n",
        "  def optimizer_step(self, epoch=None, batch_idx=None, optimizer=None, optimizer_idx=None, optimizer_closure=None, on_tpu=None, using_native_amp=None, using_lbfgs=None):\n",
        "    logger.info('optimizer_step')\n",
        "    optimizer.step(closure=optimizer_closure)\n",
        "    optimizer.zero_grad()\n",
        "    self.lr_scheduler.step()\n",
        "    \n",
        "    lr = self.lr_scheduler.get_last_lr()[0]\n",
        "    self.log('lr',lr)\n",
        "    \n",
        "  def _get_dataset(self, tokenizer, df, max_len):\n",
        "    logger.info('_get_dataset')\n",
        "    return customDataset(df=df, tokenizer=tokenizer, max_len=max_len)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    logger.info('train_dataloader')\n",
        "    train_dataset = self._get_dataset(df=self.df_train, tokenizer=self.tokenizer, max_len=self.hyparams.max_len)\n",
        "    dataloader = DataLoader(train_dataset, batch_size=self.hyparams.train_batch_size\n",
        "                            , drop_last=True, shuffle=True, num_workers=4)\n",
        "\n",
        "    dataset_len = len(dataloader.dataset)\n",
        "    train_batch_mul_gpus = (\n",
        "        self.hyparams.train_batch_size * max(1, self.hyparams.n_gpu))\n",
        "    accumulate_grad_batches = self.trainer.accumulate_grad_batches\n",
        "    train_epochs = float(self.hyparams.epochs)\n",
        "    t_total = int((dataset_len // train_batch_mul_gpus) //\n",
        "              accumulate_grad_batches * train_epochs)\n",
        "    # Generate scheduler for linear\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        self.opt, num_warmup_steps=self.hyparams.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "    self.lr_scheduler = scheduler      \n",
        "    return dataloader\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    logger.info('val_dataloader')\n",
        "    val_dataset = self._get_dataset(df=self.df_test, tokenizer=self.tokenizer, max_len=self.hyparams.max_len)\n",
        "    return DataLoader(val_dataset, batch_size=self.hyparams.eval_batch_size\n",
        "                      , num_workers=4)"
      ],
      "metadata": {
        "id": "m-RbV6CJ-jmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 't5-small'\n",
        "\n",
        "hparams = dict(\n",
        "    epochs=3\n",
        "    , model_name=checkpoint\n",
        "    , max_len=512\n",
        "    , learning_rate=3e-4\n",
        "    , weight_decay=0.0\n",
        "    , adam_epsilon=1e-8\n",
        "    , gradient_accumulation_steps=1\n",
        "    , warmup_steps=0\n",
        "    , train_batch_size=8\n",
        "    , eval_batch_size=8\n",
        "    , fp_16=False\n",
        "    , opt_level='01' # for more information see https://nvidia.github.io/apex/amp.html#opt-levels\n",
        "    , max_grad_norm=1.0\n",
        "    , seed=42\n",
        "    , n_gpu = 1\n",
        "    )\n",
        "model_args = argparse.Namespace(**hparams)\n",
        "model = T5FineTuner(build_20newsgroups_dataset(),model_args)"
      ],
      "metadata": {
        "id": "VYYo4YCl_mz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zjk_vxHoSMM9",
        "outputId": "1a298a17-35fe-4bc0-ff2d-3818d9a03c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text     label\n",
              "15505  What is hardware handshaking and when do I wan...      tech\n",
              "13325  \\nAfter reading my local paper today, I found ...    sports\n",
              "1070   Old Nick (the little devil!) wibbles:-\\n%\\n% T...    sports\n",
              "16016  \\n  If the Anne Frank exhibit makes it to your...  religion\n",
              "6596   \\n\\nIDE does not do DMA.  This is because it's...      tech"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae326e56-3098-4b61-9d70-dfa5f3cab5fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15505</th>\n",
              "      <td>What is hardware handshaking and when do I wan...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13325</th>\n",
              "      <td>\\nAfter reading my local paper today, I found ...</td>\n",
              "      <td>sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1070</th>\n",
              "      <td>Old Nick (the little devil!) wibbles:-\\n%\\n% T...</td>\n",
              "      <td>sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16016</th>\n",
              "      <td>\\n  If the Anne Frank exhibit makes it to your...</td>\n",
              "      <td>religion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6596</th>\n",
              "      <td>\\n\\nIDE does not do DMA.  This is because it's...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae326e56-3098-4b61-9d70-dfa5f3cab5fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae326e56-3098-4b61-9d70-dfa5f3cab5fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae326e56-3098-4b61-9d70-dfa5f3cab5fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGYdAAFdsSNc",
        "outputId": "1b8ba70c-53d0-4621-a355-8becb7578d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-0ffaf8da-aba8-8627-aaa1-706856d9d796)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = argparse.Namespace(**hparams)\n",
        "\n",
        "train_params = dict(\n",
        "    #accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    accelerator='gpu',\n",
        "    devices=1,\n",
        "    max_epochs=args.epochs,\n",
        "    #early_stop_callback=False,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    amp_backend='apex',\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        "    #logger=tb_logger,\n",
        "    #callbacks=lr_monitor,\n",
        "    #enable_checkpointing=checkpoint_callback,\n",
        "    #callbacks=[LoggingCallback()],\n",
        ")"
      ],
      "metadata": {
        "id": "E13YvHUQrU1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(**train_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGCo-wkbQtPx",
        "outputId": "33d9ca80-ca19-46f8-ae7b-8c271177411d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/lightning_logs\n",
        "trainer.fit(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "Qrgs8Zeqsr6q",
        "outputId": "34f37bde-1be8-4229-d8ac-6ee2bdfb0fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bc52647eb0ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir /content/lightning_logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYTl9UDptBxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOs7ReyZg7dXT8ihKu3HDAU",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}